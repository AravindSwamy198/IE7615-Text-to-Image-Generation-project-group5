ğŸŒŸ README â€” End-to-End CLIP + Stable Diffusion Pipeline with Fine-Tuning & Evaluation

Welcome to the Text-to-Image Generation & Fine-Tuning Project! ğŸš€
This repository implements a complete, production-grade workflow that combines COCO Dataset preprocessing, CLIP text embeddings, Stable Diffusion generation, domain-specific fine-tuning, and quantitative evaluation using FID & Inception Score.
It includes dataset extraction, training, inference, optimization, and deep integration between CLIP and the Stable Diffusion UNet.

ğŸ§© Project Overview

This project walks through every major stage of building a high-performance text-to-image generator:

ğŸ”½ Download & explore COCO Image Captioning dataset

ğŸ–¼ï¸ Build imageâ€“caption pairs (400k+)

âœ‚ï¸ Extract domain-specific subsets (Animals / Vehicles / Food / Sports / Indoor)

ğŸ§¹ Resize, clean, and prepare images for model input

ğŸ§  Generate CLIP embeddings for all captions

ğŸ¨ Load Stable Diffusion v1-4 for text-to-image

ğŸ”— Verify CLIP â†’ UNet cross-attention integration

ğŸ¯ Fine-tune Stable Diffusion on animal domain

ğŸ“Š Evaluate model quality with proper FID and IS metrics

âš™ï¸ Compare schedulers & CFG scales to pick the best combination

ğŸ¦ Generate beautiful images using your fine-tuned model

Every stage is automated, logged, visualized, and stored in structured folders for reproducibility. ğŸ’¾

ğŸ“¦ 1. Dataset Acquisition & Preprocessing

The project begins by pulling the COCO Dataset via KaggleHub:

Over 400,000 captionâ€“image pairs

Multiple annotations directories

Images processed into 256Ã—256 RGB

All invalid/missing images removed

Captions stored in captions_subset.json

A domain-filtering system lets you choose categories like:

ğŸ¶ Animals
ğŸš— Vehicles
ğŸ• Food
ğŸ€ Sports
ğŸ›‹ï¸ Indoor

For this project, the Animals domain was selected.
More than 69,000 captioned animal images were detected, and 4,000 samples were used for training-ready preprocessing.

ğŸ§  2. CLIP Text Embedding Generation

The project loads OpenAIâ€™s CLIP ViT-B/32:

Tokenizes captions

Implements mean pooling

Generates 512-dim embeddings per caption

Saves embeddings to:

data/processed/embeddings/text_embeddings.npy

data/processed/embeddings/text_index.json

We verify embedding meaningfulness via cosine similarity (e.g., closely related captions yield high similarity). ğŸ”

This embedding matrix becomes the conditioning signal for Stable Diffusion.

ğŸ¨ 3. Stable Diffusion Integration

Stable Diffusion v1-4 is loaded with:

âš¡ FP16 precision

ğŸ§© Attention slicing

ğŸ›‘ Safety checker disabled for faster inference

Then we run Milestone-1: generate sample images for prompts like:

â€œA golden retriever playing fetch on a sunny beachâ€

â€œA futuristic neon-lit city skyline at nightâ€

â€œFresh sushi on a wooden plate with chopsticksâ€

Generation time is around 8â€“9 seconds per image on Tesla P100 GPU. âš¡

ğŸ”— 4. Deep CLIP â†” Diffusion Verification

We confirm:

CLIP generates 77 Ã— 768 embeddings

These embeddings flow into 16 UNet cross-attention layers

Latents & embeddings match dtype and shape

UNet uses CLIP conditioning during denoising

This verification ensures the foundation for fine-tuning is correct, stable, and fully integrated. âœ”ï¸

ğŸ¾ 5. Domain-Specific Fine-Tuning (Animals)

We fine-tune only the text-conditioning attention layers (attn2.to_v), keeping 850M+ parameters frozen.

Advantages:

ğŸ”¥ Low GPU memory

ğŸš€ Fast training

ğŸ›¡ï¸ Stable convergence

ğŸ˜ Strong domain specialization

Breakdown:

2,000 animal images

2 epochs (~4,000 steps)

SGD optimizer

FP16 + gradient checkpointing

Auto-casting for speed

Timesteps sampled from 0â€“999

Training logs include:

Loss curves

Memory usage

Speed (1.6â€“1.7 images/sec)

Checkpoints every 100 steps

Final avg loss ~0.23 ğŸ“‰

Everything is saved in:

models/fine_tuned_animals/


Including:

fine-tuned UNet

full pipeline

training metadata

ğŸ¦Š 6. Generating Images with the Fine-Tuned Model

After training, prompts like:

â€œA majestic lion in the savannaâ€

â€œA colorful parrot on a branchâ€

â€œA playful puppy in a gardenâ€

produce much sharper, more domain-aware, and more consistent images than the base model. ğŸ‰

A comparison plot includes:

ğŸ“‰ Training loss curve

ğŸ¦ Three sample generated images

ğŸ“Š 7. FID & IS Evaluation (Proper Implementation)

We implement correct versions of:

ğŸ”¢ FrÃ©chet Inception Distance (FID)

Using:

Mean of features

Covariance matrices

âˆš product covariance

Trace operations

ğŸ§ª Inception Score (IS)

Using KL divergence across splits.

ğŸ§¬ 8. Scheduler + CFG Experiment Grid

We test:

Schedulers

DDIM

PNDM

Euler

CFG scales

5.0

7.5

10.0

Settings

Steps: 40

Images per config: 40

Prompt: â€œA photorealistic orange tabby catâ€¦â€

A reference set (40 images) is generated using DPMSolver.
Then each schedulerâ€“CFG combo is evaluated using:

FID score ğŸ§®

Inception Score ğŸ”¥

Results help determine:

Best realism

Best diversity

Best prompt alignment

All metrics logged, saved, and reusable.

ğŸ’¡ 9. Final Inference Mode

Users can type:

Enter your prompt:


and instantly generate images using the domain-specialized model.
On GPU, generation takes ~8 seconds per 512Ã—512 image.

ğŸ Project Highlights

âœ¨ Complete pipeline from dataset â†’ embeddings â†’ diffusion â†’ fine-tuning
ğŸ§  Intelligent domain filtering
ğŸ“¦ Efficient data preprocessing pipeline
ğŸ§¬ CLIP + Diffusion integration verified
ğŸ”¥ Fine-tuning with only 16 parameters
ğŸ“Š Full evaluation using proper FID & IS
ğŸ¦ Domain-optimized final model
ğŸ› ï¸ Best-in-class scheduler comparison
ğŸš€ Fast inference & clean model deployment structure

ğŸ‰ Conclusion

This project delivers a fully functional, fine-tuned, and evaluated text-to-image systemâ€”capable of generating high-quality animal images conditioned on natural-language prompts.
It demonstrates mastery across:

Large-scale data processing

Embedding extraction

Latent diffusion modeling

GPU-optimized training

Advanced evaluation metrics

Domain specialization

You now have a pipeline that can be adapted to ANY domain and ANY dataset, including LoRA, DreamBooth, or multi-domain fine-tuning.
